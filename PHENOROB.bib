
@article{bates_estimating_2021,
	title = {Estimating {Canopy} {Density} {Parameters} {Time}-{Series} for {Winter} {Wheat} {Using} {UAS} {Mounted} {LiDAR}},
	volume = {13},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/4/710},
	doi = {10.3390/rs13040710},
	abstract = {Monitoring of canopy density with related metrics such as leaf area index (LAI) makes a signiﬁcant contribution to understanding and predicting processes in the soil–plant–atmosphere system and to indicating crop health and potential yield for farm management. Remote sensing methods using optical sensors that rely on spectral reﬂectance to calculate LAI have become more mainstream due to easy entry and availability. Methods with vegetation indices (VI) based on multispectral reﬂectance data essentially measure the green area index (GAI) or response to chlorophyll content of the canopy surface and not the entire aboveground biomass that may be present from non-green elements that are key to fully assessing the carbon budget. Methods with light detection and ranging (LiDAR) have started to emerge using gap fraction (GF) to estimate the plant area index (PAI) based on canopy density. These LiDAR methods have the main advantage of being sensitive to both green and non-green plant elements. They have primarily been applied to forest cover with manned airborne LiDAR systems (ALS) and have yet to be used extensively with crops such as winter wheat using LiDAR on unmanned aircraft systems (UAS). This study contributes to a better understanding of the potential of LiDAR as a tool to estimate canopy structure in precision farming. The LiDAR method proved to have a high to moderate correlation in spatial variation to the multispectral method. The LiDAR-derived PAI values closely resemble the SunScan Ceptometer GAI ground measurements taken early in the growing season before major stages of senescence. Later in the growing season, when the canopy density was at its highest, a possible overestimation may have occurred. This was most likely due to the chosen ﬂight parameters not providing the best depictions of canopy density with consideration of the LiDAR’s perspective, as the ground-based destructive measurements provided lower values of PAI. Additionally, a distinction between total LiDAR-derived PAI, multispectral-derived GAI, and brown area index (BAI) is made to show how the active and passive optical sensor methods used in this study can complement each other throughout the growing season.},
	language = {en},
	number = {4},
	urldate = {2023-04-12},
	journal = {Remote Sensing},
	author = {Bates, Jordan Steven and Montzka, Carsten and Schmidt, Marius and Jonard, François},
	month = feb,
	year = {2021},
	pages = {710},
	file = {Bates 2021.pdf:C\:\\Users\\dagos\\Desktop\\JULICH\\Ressources\\Bates 2021.pdf:application/pdf},
}

@article{dandrifosse_imaging_2020,
	title = {Imaging {Wheat} {Canopy} {Through} {Stereo} {Vision}: {Overcoming} the {Challenges} of the {Laboratory} to {Field} {Transition} for {Morphological} {Features} {Extraction}},
	volume = {11},
	issn = {1664-462X},
	shorttitle = {Imaging {Wheat} {Canopy} {Through} {Stereo} {Vision}},
	url = {https://www.frontiersin.org/article/10.3389/fpls.2020.00096/full},
	doi = {10.3389/fpls.2020.00096},
	abstract = {Stereo vision is a 3D imaging method that allows quick measurement of plant architecture. Historically, the method has mainly been developed in controlled conditions. This study identiﬁed several challenges to adapt the method to natural ﬁeld conditions and propose solutions. The plant traits studied were leaf area, mean leaf angle, leaf angle distribution, and canopy height. The experiment took place in a winter wheat, Triticum aestivum L., ﬁeld dedicated to fertilization trials at Gembloux (Belgium). Images were acquired thanks to two nadir cameras. A machine learning algorithm using RGB and HSV color spaces is proposed to perform soil-plant segmentation robust to light conditions. The matching between images of the two cameras and the leaf area computation was improved if the number of pixels in the image of a scene was binned from 2560 × 2048 to 1280 × 1024 pixels, for a distance of 1 m between the cameras and the canopy. Height descriptors such as median or 95th percentile of plant heights were useful to precisely compare the development of different canopies. Mean spike top height was measured with an accuracy of 97.1 \%. The measurement of leaf area was affected by overlaps between leaves so that a calibration curve was necessary. The leaf area estimation presented a root mean square error (RMSE) of 0.37. The impact of wind on the variability of leaf area measurement was inferior to 3\% except at the stem elongation stage. Mean leaf angles ranging from 53° to 62° were computed for the whole growing season. For each acquisition date during the vegetative stages, the variability of mean angle measurement was inferior to 1.5\% which underpins that the method is precise.},
	language = {en},
	urldate = {2023-04-12},
	journal = {Frontiers in Plant Science},
	author = {Dandrifosse, Sébastien and Bouvry, Arnaud and Leemans, Vincent and Dumont, Benjamin and Mercatoris, Benoît},
	month = feb,
	year = {2020},
	pages = {96},
	file = {Dandrifosse 2020.pdf:C\:\\Users\\dagos\\Desktop\\JULICH\\Ressources\\Dandrifosse 2020.pdf:application/pdf},
}

@misc{helmrich_towards_2021,
	title = {Towards {Large}-{Scale} {Rendering} of {Simulated} {Crops} for {Synthetic} {Ground} {Truth} {Generation} on {Modular} {Supercomputers}},
	url = {http://arxiv.org/abs/2110.14946},
	abstract = {Computer Vision problems deal with the semantic extraction of information from camera images. Especially for ﬁeld crop images, the underlying problems are hard to label and even harder to learn, and the availability of high-quality training data is low. Deep neural networks do a good job of extracting the necessary models from training examples. However, they rely on an abundance of training data that is not feasible to generate or label by expert annotation. To address this challenge, we make use of the Unreal Engine to render large and complex virtual scenes. We rely on the performance of individual nodes by distributing plant simulations across nodes and both generate scenes as well as train neural networks on GPUs, restricting node communication to parallel learning.},
	language = {en},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Helmrich, Dirk Norbert and Göbbert, Jens Henrik and Giraud, Mona and Scharr, Hanno and Schnepf, Andrea and Riedel, Morris},
	month = oct,
	year = {2021},
	note = {arXiv:2110.14946 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.3, I.4, I.6},
	annote = {Comment: Accepted Poster for the 11th IEEE Symposium on Large Data Analysis and Visualization},
	file = {Helmrich 2021.pdf:C\:\\Users\\dagos\\Desktop\\JULICH\\Ressources\\Helmrich 2021.pdf:application/pdf},
}

@article{li_review_2014,
	title = {A {Review} of {Imaging} {Techniques} for {Plant} {Phenotyping}},
	volume = {14},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/14/11/20078},
	doi = {10.3390/s141120078},
	abstract = {Given the rapid development of plant genomic technologies, a lack of access to plant phenotyping capabilities limits our ability to dissect the genetics of quantitative traits. Effective, high-throughput phenotyping platforms have recently been developed to solve this problem. In high-throughput phenotyping platforms, a variety of imaging methodologies are being used to collect data for quantitative studies of complex traits related to the growth, yield and adaptation to biotic or abiotic stress (disease, insects, drought and salinity). These imaging techniques include visible imaging (machine vision), imaging spectroscopy (multispectral and hyperspectral remote sensing), thermal infrared imaging, fluorescence imaging, 3D imaging and tomographic imaging (MRT, PET and CT). This paper presents a brief review on these imaging techniques and their applications in plant phenotyping. The features used to apply these imaging techniques to plant phenotyping are described and discussed in this review.},
	language = {en},
	number = {11},
	urldate = {2023-04-12},
	journal = {Sensors},
	author = {Li, Lei and Zhang, Qin and Huang, Danfeng},
	month = oct,
	year = {2014},
	pages = {20078--20111},
	file = {Li 2014.pdf:C\:\\Users\\dagos\\Desktop\\JULICH\\Ressources\\Li 2014.pdf:application/pdf},
}

@article{li_self-supervised_2023,
	title = {Self-{Supervised} {Plant} {Phenotyping} by {Combining} {Domain} {Adaptation} with {3D} {Plant} {Model} {Simulations}: {Application} to {Wheat} {Leaf} {Counting} at {Seedling} {Stage}},
	volume = {5},
	issn = {2643-6515},
	shorttitle = {Self-{Supervised} {Plant} {Phenotyping} by {Combining} {Domain} {Adaptation} with {3D} {Plant} {Model} {Simulations}},
	url = {https://spj.science.org/doi/10.34133/plantphenomics.0041},
	doi = {10.34133/plantphenomics.0041},
	abstract = {The number of leaves at a given time is important to characterize plant growth and development. In this work, we developed a high-throughput method to count the number of leaves by detecting leaf tips in RGB images. The Digital Plant Phenotyping platform was used to simulate a large and diverse dataset of RGB images and corresponding leaf-tip labels of wheat plants at seedling stages (150,000 images with over 2 million labels). The realism of the images was then improved using domain adaptation methods before training deep learning models. The results demonstrate the efficiency of the proposed method evaluated on a diverse test dataset, collecting measurements from five countries obtained under different environments, growth stages, and lighting conditions with different cameras (450 images with over 2162 labels). Among the six combinations of deep learning models and domain adaptation techniques, the Faster-RCNN model with CycleGAN adaptation technique provided the best performance (R2 = 0.94, RMSE = 8.7). Complementary studies show that it is essential to simulate images with sufficient realism (background, leaf texture, lighting conditions) before applying domain adaptation techniques. Furthermore, the spatial resolution should be better than 0.6 mm/pixel to identify leaf tips. The method is claimed to be self-supervised since no manual labeling is required for model training. The self-supervised phenotyping approach developed here offers great potential for addressing a wide range of plant phenotyping problems. The trained networks are available at https://github.com/YinglunLi/Wheat-leaf-tip-detection.},
	language = {en},
	urldate = {2023-04-12},
	journal = {Plant Phenomics},
	author = {Li, Yinglun and Zhan, Xiaohai and Liu, Shouyang and Lu, Hao and Jiang, Ruibo and Guo, Wei and Chapman, Scott and Ge, Yufeng and Solan, Benoit and Ding, Yanfeng and Baret, Frédéric},
	month = jan,
	year = {2023},
	pages = {0041},
	file = {Li 2023.pdf:C\:\\Users\\dagos\\Desktop\\JULICH\\Ressources\\Li 2023.pdf:application/pdf},
}

@article{montzka_sensitivity_2023,
	title = {Sensitivity of {LiDAR} {Parameters} to {Aboveground} {Biomass} in {Winter} {Spelt}},
	volume = {7},
	issn = {2504-446X},
	url = {https://www.mdpi.com/2504-446X/7/2/121},
	doi = {10.3390/drones7020121},
	abstract = {Information about the current biomass state of crops is important to evaluate whether the growth conditions are adequate in terms of water and nutrient supply to determine if there is need to react to diseases and to predict the expected yield. Passive optical Unmanned Aerial Vehicle (UAV)-based sensors such as RGB or multispectral cameras are able to sense the canopy surface and record, e.g., chlorophyll-related plant characteristics, which are often indirectly correlated to aboveground biomass. However, direct measurements of the plant structure can be provided by LiDAR systems. In this study, different LiDAR-based parameters are evaluated according to their relationship to aboveground fresh and dry biomass (AGB) for a winter spelt experimental ﬁeld in Dahmsdorf, Brandenburg, Germany. The parameters crop height, gap fraction, and LiDAR intensity are analyzed according to their individual correlation with AGB, and also a multiparameter analysis using the Ordinary Least Squares Regression (OLS) is performed. Results indicate high absolute correlations of AGB with gap fraction and crop height (−0.82 and 0.77 for wet and −0.70 and 0.66 for dry AGB, respectively), whereas intensity needs further calibration or processing before it can be adequately used to estimate AGB (−0.27 and 0.22 for wet and dry AGB, respectively). An important outcome of this study is that the combined utilization of all LiDAR parameters via an OLS analysis results in less accurate AGB estimation than with gap fraction or crop height alone. Moreover, future AGB states in June and July were able to be estimated from May LiDAR parameters with high accuracy, indicating stable spatial patterns in crop characteristics over time.},
	language = {en},
	number = {2},
	urldate = {2023-04-12},
	journal = {Drones},
	author = {Montzka, Carsten and Donat, Marco and Raj, Rahul and Welter, Philipp and Bates, Jordan Steven},
	month = feb,
	year = {2023},
	pages = {121},
	file = {Montza 2023.pdf:C\:\\Users\\dagos\\Desktop\\JULICH\\Ressources\\Montza 2023.pdf:application/pdf},
}

@article{morandage_bayesian_2021,
	title = {Bayesian inference of root architectural model parameters from synthetic field data},
	volume = {467},
	issn = {0032-079X, 1573-5036},
	url = {https://link.springer.com/10.1007/s11104-021-05026-4},
	doi = {10.1007/s11104-021-05026-4},
	abstract = {Background and aims  Characterizing root system architectures of field-grown crops is challenging as root systems are hidden in the soil. We investigate the possibility of estimating root architecture model parameters from soil core data in a Bayesian framework.},
	language = {en},
	number = {1-2},
	urldate = {2023-04-12},
	journal = {Plant and Soil},
	author = {Morandage, Shehan and Laloy, Eric and Schnepf, Andrea and Vereecken, Harry and Vanderborght, Jan},
	month = oct,
	year = {2021},
	pages = {67--89},
	file = {Morandage 2021.pdf:C\:\\Users\\dagos\\Desktop\\JULICH\\Ressources\\Morandage 2021.pdf:application/pdf},
}

@article{zheng_retrieving_2009,
	title = {Retrieving {Leaf} {Area} {Index} ({LAI}) {Using} {Remote} {Sensing}: {Theories}, {Methods} and {Sensors}},
	volume = {9},
	issn = {1424-8220},
	shorttitle = {Retrieving {Leaf} {Area} {Index} ({LAI}) {Using} {Remote} {Sensing}},
	url = {http://www.mdpi.com/1424-8220/9/4/2719},
	doi = {10.3390/s90402719},
	abstract = {The ability to accurately and rapidly acquire leaf area index (LAI) is an indispensable component of process-based ecological research facilitating the understanding of gas-vegetation exchange phenomenon at an array of spatial scales from the leaf to the landscape. However, LAI is difficult to directly acquire for large spatial extents due to its time consuming and work intensive nature. Such efforts have been significantly improved by the emergence of optical and active remote sensing techniques. This paper reviews the definitions and theories of LAI measurement with respect to direct and indirect methods. Then, the methodologies for LAI retrieval with regard to the characteristics of a range of remotely sensed datasets are discussed. Remote sensing indirect methods are subdivided into two categories of passive and active remote sensing, which are further categorized as terrestrial, aerial and satellite-born platforms. Due to a wide variety in spatial resolution of remotely sensed data and the requirements of ecological modeling, the scaling issue of LAI is discussed and special consideration is given to extrapolation of measurement to landscape and regional levels.},
	language = {en},
	number = {4},
	urldate = {2023-04-12},
	journal = {Sensors},
	author = {Zheng, Guang and Moskal, L. Monika},
	month = apr,
	year = {2009},
	pages = {2719--2745},
	file = {Zheng 2009.pdf:C\:\\Users\\dagos\\Desktop\\JULICH\\Ressources\\Zheng 2009.pdf:application/pdf},
}

@misc{bouvry_digital_2023,
	address = {Berlin},
	title = {Digital {Twin} of a {Smart} {Plant} {Factory} for {Plant} {Phenotyping} : {Data} assimilation between measured and simulated {3D} point cloud data in the {CPlantBox} {FSPM}},
	author = {Bouvry, Arnaud},
	collaborator = {Lebeau, Frédéric},
	year = {2023},
	file = {Bouvry 2023.pdf:C\:\\Users\\dagos\\Zotero\\storage\\XKMRX9UC\\Bouvry 2023.pdf:application/pdf},
}

@article{bates_machine_2022,
	title = {Machine {Learning} with {UAS} {LiDAR} for {Winter} {Wheat} {Biomass} {Estimations}},
	volume = {3},
	issn = {2700-8150},
	url = {https://agile-giss.copernicus.org/articles/3/23/2022/},
	doi = {10.5194/agile-giss-3-23-2022},
	abstract = {Biomass is an important indicator in the ecological and management process that can now be estimated at higher temporal and spatial resolutions because of unmanned aircraft systems (UAS). LiDAR sensor technology has advanced enabling more compact sizes that can be integrated with UAS platforms. Its signals are capable of penetrating through vegetation canopies enabling the capture of more information along the plant structure. Separate studies have used LiDAR for crop height, rate of canopy penetrations as related to leaf area index (LAI), and signal intensity as an indicator of plant chlorophyll status or green area index (GAI). These LiDAR products are combined within a machine learning method such as an artificial neural network (ANN) to assess the potential in making accurate biomass estimations for winter wheat.},
	language = {en},
	urldate = {2023-11-21},
	journal = {AGILE: GIScience Series},
	author = {Bates, Jordan and Jonard, Francois and Bajracharya, Rajina and Vereecken, Harry and Montzka, Carsten},
	month = jun,
	year = {2022},
	pages = {1--4},
	file = {Bates et al. - 2022 - Machine Learning with UAS LiDAR for Winter Wheat B.pdf:C\:\\Users\\dagos\\Zotero\\storage\\95AVUQAR\\Bates et al. - 2022 - Machine Learning with UAS LiDAR for Winter Wheat B.pdf:application/pdf},
}

@misc{schnepfa_plant-root-soil-interactions-modellingcplantbox_2022,
	title = {Plant-{Root}-{Soil}-{Interactions}-{Modelling}/{CPlantBox}: {CPlantBox}},
	copyright = {Open Access},
	shorttitle = {Plant-{Root}-{Soil}-{Interactions}-{Modelling}/{CPlantBox}},
	url = {https://zenodo.org/record/6953939},
	abstract = {No description provided.},
	urldate = {2023-12-21},
	publisher = {Zenodo},
	author = {{Schnepfa}},
	month = aug,
	year = {2022},
	doi = {10.5281/ZENODO.6953939},
}

@book{schnepfa2022,
	title = {Plant-Root-Soil-Interactions-Modelling/CPlantBox: CPlantBox},
	author = {Schnepfa, },
	year = {2022},
	month = {08},
	date = {2022-08-02},
	publisher = {Zenodo},
	doi = {10.5281/ZENODO.6953939},
	url = {https://zenodo.org/record/6953939},
	note = {DOI: 10.5281/ZENODO.6953939}
}

@article{zhou2020,
	title = {CPlantBox, a whole-plant modelling framework for the simulation of water- and carbon-related processes},
	author = {Zhou, Xiao-Ran and Schnepf, Andrea and Vanderborght, Jan and Leitner, Daniel and Lacointe, {André} and Vereecken, Harry and Lobet, Guillaume},
	year = {2020},
	month = {01},
	date = {2020-01-01},
	journal = {in silico Plants},
	volume = {2},
	number = {1},
	doi = {10.1093/insilicoplants/diaa001},
	url = {https://academic.oup.com/insilicoplants/article/doi/10.1093/insilicoplants/diaa001/5709632},
	langid = {en}
}
